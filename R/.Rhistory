#' @param cutoff a cutoff for classification models, needed for measures like recall, precision, ACC, F1. By default 0.5.
#'
#' @return An object of the class \code{model_performance}.
#'
#' It's a list with following fields:
#'
#' \itemize{
#' \item \code{residuals} - data frame that contains residuals for each observation
#' \item \code{measures} - list with calculated measures that are dedicated for the task, whether it is regression, binary classification or multiclass classification.
#' \item \code{type} - character that specifies type of the task.
#' }
#'
#' @references Explanatory Model Analysis. Explore, Explain, and Examine Predictive Models. \url{https://ema.drwhy.ai/}
#' @importFrom stats median weighted.mean
#' @export
#' @examples
#' \donttest{
#' # regression
#'
#' library("ranger")
#' apartments_ranger_model <- ranger(m2.price~., data = apartments, num.trees = 50)
#' explainer_ranger_apartments  <- explain(apartments_ranger_model, data = apartments[,-1],
#'                              y = apartments$m2.price, label = "Ranger Apartments")
#' model_performance_ranger_aps <- model_performance(explainer_ranger_apartments )
#' model_performance_ranger_aps
#' plot(model_performance_ranger_aps)
#' plot(model_performance_ranger_aps, geom = "boxplot")
#' plot(model_performance_ranger_aps, geom = "histogram")
#'
#' # binary classification
#'
#' titanic_glm_model <- glm(survived~., data = titanic_imputed, family = "binomial")
#' explainer_glm_titanic <- explain(titanic_glm_model, data = titanic_imputed[,-8],
#'                          y = titanic_imputed$survived)
#' model_performance_glm_titanic <- model_performance(explainer_glm_titanic)
#' model_performance_glm_titanic
#' plot(model_performance_glm_titanic)
#' plot(model_performance_glm_titanic, geom = "boxplot")
#' plot(model_performance_glm_titanic, geom = "histogram")
#'
#' # multilabel classification
#'
#' HR_ranger_model <- ranger(status~., data = HR, num.trees = 50,
#'                                probability = TRUE)
#' explainer_ranger_HR  <- explain(HR_ranger_model, data = HR[,-6],
#'                              y = HR$status, label = "Ranger HR")
#' model_performance_ranger_HR <- model_performance(explainer_ranger_HR)
#' model_performance_ranger_HR
#' plot(model_performance_ranger_HR)
#' plot(model_performance_ranger_HR, geom = "boxplot")
#' plot(model_performance_ranger_HR, geom = "histogram")
#'
#'}
#'
model_performancex <- function(explainer, ..., cutoff = 0.5) {
#test_explainer(explainer, has_data = TRUE, has_y = TRUE, function_name = "model_performance")
# Check since explain could have been run with precalculate = FALSE
if (is.null(explainer$y_hat)) {
predicted <- explainer$predict_function(explainer$model, explainer$data, ...)
} else {
predicted <- explainer$y_hat
}
observed <- explainer$y
# Check since explain could have been run with precalculate = FALSE
if (is.null(explainer$residuals)) {
diff <- explainer$residual_function(explainer$model, explainer$data, observed, explainer$predict_function)
} else {
# changed according to #130
diff <- explainer$residuals
}
residuals <- data.frame(predicted, observed, diff = diff)
# get proper measures
type <- explainer$model_info$type
if (type == "regression") {
measures <- list(
mse = model_performance_mse(predicted, observed),
rmse = model_performance_rmse(predicted, observed),
r2 = model_performance_r2(predicted, observed),
mad = model_performance_mad(predicted, observed)
)
} else if (type == "classification") {
tp = sum((observed == 1) * (predicted >= cutoff))
fp = sum((observed == 0) * (predicted >= cutoff))
tn = sum((observed == 0) * (predicted < cutoff))
fn = sum((observed == 1) * (predicted < cutoff))
measures <- list(
recall    = model_performance_recall(tp, fp, tn, fn),
precision = model_performance_precision(tp, fp, tn, fn),
f1        = model_performance_f1(tp, fp, tn, fn),
accuracy  = model_performance_accuracy(tp, fp, tn, fn),
auc       = model_performance_auc(predicted, observed),
mcc               = model_performance_mcc(tp, fp, tn, fn),
brier_score       = model_performance_brier(predicted, observed),
log_loss          = model_performance_logloss(predicted, observed),
balanced_accuracy = model_performance_bacc(tp, fp, tn, fn)
)
} else if (type == "multiclass") {
measures <- list(
micro_F1 = model_performance_micro_f1(predicted, observed),
macro_F1 = model_performance_macro_f1(predicted, observed),
w_macro_F1 = model_performance_weighted_macro_f1(predicted, observed),
accuracy = model_performance_accuracy_multi(predicted, observed),
w_macro_auc = model_performance_weighted_macro_auc(predicted, observed)
)
} else {
stop("`explainer$model_info$type` should be one of ['regression', 'classification', 'multiclass'] - pass `model_info = list(type = $type$)` to the `explain` function. Submit an issue on https://github.com/ModelOriented/DALEX/issues if you think that this model should be covered by default.")
}
residuals$label <- explainer$label
structure(list(residuals, measures, type),
.Names = c("residuals", "measures", "type"),
class = "model_performance")
}
model_performance_mse <- function(predicted, observed) {
mean((predicted - observed)^2, na.rm = TRUE)
}
model_performance_rmse <- function(predicted, observed) {
sqrt(mean((predicted - observed)^2, na.rm = TRUE))
}
model_performance_r2 <- function(predicted, observed) {
1 - model_performance_mse(predicted, observed)/model_performance_mse(mean(observed), observed)
}
model_performance_mad <- function(predicted, observed) {
median(abs(predicted - observed))
}
model_performance_auc <- function(predicted, observed) {
tpr_tmp <- tapply(observed, predicted, sum)
TPR <- c(0,cumsum(rev(tpr_tmp)))/sum(observed)
fpr_tmp <- tapply(1 - observed, predicted, sum)
FPR <- c(0,cumsum(rev(fpr_tmp)))/sum(1 - observed)
auc <- sum(diff(FPR)*(TPR[-1] + TPR[-length(TPR)])/2)
auc
}
model_performance_recall <- function(tp, fp, tn, fn) {
tp/(tp + fn)
}
model_performance_precision <- function(tp, fp, tn, fn) {
tp/(tp + fp)
}
model_performance_f1 <- function(tp, fp, tn, fn) {
recall = tp/(tp + fn)
precision = tp/(tp + fp)
2 * (precision * recall)/(precision + recall)
}
model_performance_accuracy <- function(tp, fp, tn, fn) {
(tp + tn)/(tp + fp + tn + fn)
}
model_performance_bacc <- function(tp, fp, tn, fn) {
((tp / (tp + fn)) + (tn / (tn + fp))) / 2
}
model_performance_logloss <- function(predicted, observed) {
predicted[which(predicted <= 0)] <- 10^-15
predicted[which(predicted >= 1)] <- 1-10^-15
-mean((observed * log(predicted) + (1 - observed) * log(1 - predicted)))
}
model_performance_brier <- function(predicted, observed) {
mean((predicted - observed) ^ 2)
}
model_performance_mcc <- function(tp, fp, tn, fn) {
tp <- as.numeric(tp)
fp <- as.numeric(fp)
tn <- as.numeric(tn)
fn <- as.numeric(fn)
((tp *  tn) - (fp * fn)) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
}
model_performance_macro_f1 <- function(predicted, observed) {
predicted_vectorized <- turn_probs_into_vector(predicted)
confusion_matrixes <- calculate_confusion_matrixes(predicted_vectorized, observed)
f1_scores <- sapply(confusion_matrixes, function(x){
model_performance_f1(x$tp, x$fp, x$tn, x$fn)
})
mean(f1_scores)
}
model_performance_micro_f1 <- function(predicted, observed) {
# For case where each point can be assigned only to one class micro_f1 equals acc
model_performance_accuracy_multi(predicted, observed)
}
model_performance_weighted_macro_f1 <- function(predicted, observed) {
predicted_vectorized <- turn_probs_into_vector(predicted)
confusion_matrixes <- calculate_confusion_matrixes(predicted_vectorized, observed)
f1_scores <- sapply(confusion_matrixes, function(x){
model_performance_f1(x$tp, x$fp, x$tn, x$fn)
})
weighted.mean(f1_scores, prop.table(table(observed))[names(confusion_matrixes)])
}
model_performance_accuracy_multi <- function(predicted, observed) {
predicted_vectorized <- turn_probs_into_vector(predicted)
mean(predicted_vectorized == observed)
}
model_performance_weighted_macro_auc <- function(predicted, observed) {
observed <- as.character(observed)
auc_scores <- sapply(unique(observed), function(x){
model_performance_auc(predicted[,x], as.numeric(observed == x))
})
weighted.mean(auc_scores, prop.table(table(observed))[unique(observed)])
}
turn_probs_into_vector <- function(observed) {
apply(observed, 1, function(x){
colnames(observed)[which.max(x)]
})
}
calculate_confusion_matrixes <- function(predicted, observed) {
observed <- as.character(observed)
ret <- lapply(unique(observed), function(x){
tp <- mean(predicted[predicted == x] == observed[predicted == x])
fp <- mean(predicted[predicted == x] != observed[predicted == x])
tn <- mean(predicted[predicted != x] == observed[predicted != x])
fn <- mean(predicted[predicted != x] != observed[predicted != x])
list(tp = ifelse(is.nan(tp), 0, tp),
fp = ifelse(is.nan(fp), 0, fp),
tn = ifelse(is.nan(tn), 0, tn),
fn = ifelse(is.nan(fn), 0, fn))
})
names(ret) <- unique(observed)
ret
}
set.seed(123)
original_model <- forester(data   = train_data,
target = "status",
type   = "classification")
#' Function for evaluating and comparing models
#'
#' Function \code{evaluate} enables user to compare one or several models with
#' the same type of task. Function compares them based on metric which can be chosen by the user
#' and returns the best model
#'
#' @param ... models to compare. Models should be either \code{forester_model} or \code{explainer} object.
#' @param data_test data frame for evaluation models.
#' @param target character indicating the target column in data test table.
#' @param metric string containing name of metric based on which the model will be selected.
#' Metric should be written in lowercase letters. Can be omitted.
#'
#'
#' @return A list containing an object of the class \code{forester_model} and data frame with results.
#'
#' @references Explanatory Model Analysis. Explore, Explain, and Examine Predictive Models. \url{https://ema.drwhy.ai/}
#' @importFrom stats median weighted.mean
#' @export
#' @examples
#' \donttest{
#' library(DALEX)
#' data(apartments, package="DALEX")
#' lightgbm_model <- make_lightgbm(apartments, "m2.price", "regression")
#' ranger_model <- make_ranger(apartments, "m2.price", "regression")
#'
#' # Evaluating single model
#' model_evaluated <- evaluate(ranger_model,
#'                              data_test = apartments,
#'                              target = "m2.price",
#'                              metric = "mse")
#'
#'
#' best_model <- evaluate(lightgbm_model, ranger_model,
#'                              data_test = apartments,
#'                              target = "m2.price",
#'                              metric = "rmse")
#'}
##
evaluate <- function(..., data_test, target, metric = NULL){
models <- list(...)
### Starting conditions
if (length(models) == 0){
stop("List of models is empty.")
}
# Check for NULL objects coming from forester()
models <- models[!sapply(models, is.null)]
# Variable for checking models type
if (!any(class(models[[1]]) %in% c("forester_model", "explainer"))){
stop("Wrong class of model. Models should have forester_model or explainer class.")
}
models_type <- models[[1]]$model_info$type
# Checking test set
data_test <- check_conditions(data = data_test,
target = target,
type = models_type)
# Data frame for final results
results <- data.frame()
### Creating data frame with all models and metrics
for (m in models){
if (!any(class(m) %in% c("forester_model", "explainer"))){
stop("Wrong class of model. Models should have forester_model or explainer class.")
}
if (m$model_info$type != models_type){
stop("All model should have the same type: classification or regression.
Can not compare models with different types")
}
### Transforming target column
steps <- m$modifications$steps[[1]]
if (any(class(steps) == "step_naomit")){
data_test <- na.omit(data_test)
}
# uploading data test
m <- DALEX::update_data(m,
data_test[, -which(names(data_test) == target)],
data_test[[target]],
verbose = FALSE)
# calculating metrics
mp <- DALEX::model_performance(m)
row <- data.frame(m$label, mp$measures)
results <- rbind(results, row)
}
colnames(results)[1] <- "model"
metric <- check_metric(metric, models_type)
class(results) <- c("forester_results", "data.frame")
#### Printing part
# Additional variable for metrics which are supposed to be minimalized
ifelse(metric %in% c("mse", "rmse", "mad"), negative <- -1, negative  <- 1)
### Choosing the best model
chosen_metric <- negative * results[[metric]]
best_model <- models[[which.max(chosen_metric)]]
print(results, metric)
message("The best model based on ", metric, " metric is ", best_model$label, ".")
return(list(best_model = best_model,
model1 = models[[1]],
model2 = models[[2]],
model3 = models[[3]],
model4 = models[[4]],
results = results))
}
print.forester_results <- function(results, metric = NULL){
if (is.null(metric)){
message("Results of compared models:")
print(knitr::kable(results, row.names = FALSE, "simple"))
} else {
# Additional variable for metrics which are supposed to be minimalized
ifelse(metric %in% c("mse", "rmse", "mad"), negative <- -1, negative  <- 1)
#### display table function
# Changing order of columns so that chosen metric is first
# Checking if column with chosen metric is on second place
if (which(names(results) == metric) != 2){
non_imprtant_metrics <- colnames(results)[!names(results) %in% c(metric, "model")]
col_names <- c("model", metric, non_imprtant_metrics)
results <- results[col_names]
}
# Sorting results based on chosen metric
final <- results[order(results[[metric]],
decreasing = as.logical(TRUE + negative)),]
# Printing table
message("Results of compared models:")
print(knitr::kable(final, row.names = FALSE, "simple"))
}
}
set.seed(123)
original_model <- forester(data   = train_data,
target = "status",
type   = "classification")
set.seed(123)
under_model <- forester(data    = under_train_data$data,
target  = "status",
type    = "classification",
refclass = "")
set.seed(123)
original_model <- forester(data   = train_data,
target = "status",
type   = "classification")
set.seed(123)
original_model <- forester(data   = train_data,
target = "status",
type   = "classification")
#' Automated Machine Learning Model Solver
#'
#' Different tree-based models such as: XGBoost, ranger, CatBoost, LightGBM, etc. require different syntaxes and
#' different specific data objects. This function provides a simple and unified formula to create those models
#' with options to automatically cover the whole process of creating Machine Learning Model: Preprocessing Data,
#' Feature Engineering, Creating Models, Optimizing Hyperparameters, Model Explanation and Evaluating Models.
#'
#'
#' @param data data.frame, matrix, data.table or dgCMatrix - training data set to create model, if data_test = NULL, then data will be
#' automatically divided into training and testing dataset. NOTE: data has to contain the target column.
#' @param target character: name of the target column, should be character and has to be column name in data.
#' @param type character: defining the task. Two options are: "regression" and "classification", particularly, binary classification.
#' @param metric character, name of metric used for evaluating best model. For regression, options are: "mse", "rmse", "mad" and "r2".
#' For classification, options are: "auc", "recall", "precision", "f1" and "accuracy".
#' @param data_test optional argument, class of data.frame, matrix, data.table or dgCMatrix - test data set used for evaluating model performance.
#' @param train_ratio numeric, ranged from between 0 and 1, indicating the proportion of splitting data train over original dataset, the remained data as data test would be used for measuring model-performance.
#' @param fill_na logical, default is FALSE. If TRUE, missing values in target column are removed, missing values in categorical columns are replaced by mode and
#' missing values in numeric columns are substituted by median of corresponding columns.
#' @param num_features numeric, default is NULL. Parameter indicates number of most important features, which are chosen from the train dataset. Automatically, those important
#' features will be kept in the train and test datasets.
#' @param tune logical. If TRUE, function will perform the hyperparameter tuning steps for each model inside.
#' @param tune_iter number (default: 20) - total number of times the optimization step is to repeated. This argument is used when tune = TRUE.
#'
#'
#' @return An object of the class \code{forester_model} which is the best model with respect to the
#' chosen metric. It's also an object of the class \code{explainer} from DALEX family inherited the
#' explanation for the best chosen model.
#'
#' @export
#' @importFrom stats predict
#' @examples
#' \donttest{
#' # regression
#' library(DALEX)
#' data(apartments, package="DALEX")
#'
#' exp <- forester(apartments, "m2.price", "regression")
#' # plot(model_parts(exp))
#'
#' # binary classification
#' library(DALEX)
#' data(titanic_imputed, package="DALEX")
#'
#' exp <- forester(titanic_imputed, "survived", "classification")
#' # plot(model_parts(exp))
#'}
##
forester <- function(data, target, type, metric = NULL, data_test = NULL, train_ratio = 0.8, fill_na = TRUE, num_features = NULL, tune = FALSE, tune_iter = 20, refclass = NULL){
message("__________________________")
message("FORESTER")
data <- check_condition(data, target, type)
### If data_test is blank, it is needed to split data into data_train and data_test by the train_ratio
if (is.null(data_test)){
splited_data <- split_data(data, target, type, ratio = train_ratio)
data_train <- splited_data[[1]]
data_test <- splited_data[[2]]
} else {
data_test <- check_condition(data_test, target, type)
data_train <- data
# Check structure of data_test:
if (!(setequal(colnames(data_train),colnames(data_test)))){
stop("Column names in train data set and test data set are not identical.")
}
}
data_for_messages <- prepare_data(data_train = data_train, target = target, type = type,
fill_na = fill_na, num_features = num_features)
message("__________________________")
message("CREATING MODELS")
### Creating models, checking for the installed packages
is_available_ranger <- try(
suppressMessages(ranger_exp <- make_ranger(data = data_train, target = target, type = type,
tune = tune, tune_metric = metric,
tune_iter = tune_iter, fill_na = fill_na,
num_features = num_features,
refclass = refclass)),
silent = TRUE
)
if (class(is_available_ranger) == "try-error") {
ranger_exp <- NULL
message("--- Omitting `make_ranger()` because the `ranger` package is not available ---")
} else {
message("--- Ranger model has been created ---")
}
is_available_catboost <- try(
suppressMessages(catboost_exp <- make_catboost(data = data_train, target = target, type = type,
tune = tune, tune_metric = metric,
tune_iter = tune_iter, fill_na = fill_na,
num_features = num_features)),
silent = TRUE
)
if (class(is_available_catboost) == "try-error") {
catboost_exp <- NULL
message("--- Omitting `make_catboost()` because the `catboost` package is not available ---")
} else {
message("--- Catboost model has been created ---")
}
is_available_xgboost <- try(
suppressMessages(xgboost_exp  <- make_xgboost(data = data_train, target = target, type = type,
tune = tune, tune_metric = metric,
tune_iter = tune_iter, fill_na = fill_na,
num_features = num_features)),
silent = TRUE
)
if (class(is_available_xgboost) == "try-error") {
xgboost_exp <- NULL
message("--- Omitting `make_xgboost()` because the `xgboost` package is not available ---")
} else {
message("--- Xgboost model has been created ---")
}
is_available_lightgbm <- try(
suppressMessages(lightgbm_exp <- make_lightgbm(data = data_train, target = target, type = type,
tune = tune, tune_metric = metric,
tune_iter = tune_iter, fill_na = fill_na,
num_features = num_features)),
silent = TRUE
)
if (class(is_available_lightgbm) == "try-error") {
lightgbm_exp <- NULL
message("--- Omitting `make_lightgbm()` because the `lightgbm` package is not available ---")
} else {
message("--- LightGBM model has been created ---")
}
message("__________________________")
message("COMPARISON")
result <- evaluate(
catboost_exp,
xgboost_exp,
ranger_exp,
lightgbm_exp,
data_test = data_test,
target = target,
metric = metric
)
return(list(best_model = result$best_model,
model1     = result$model1,
model2     = result$model2,
model3     = result$model3,
model4     = result$model4,
test_data  = data_test))
}
set.seed(123)
original_model <- forester(data   = train_data,
target = "status",
type   = "classification")
install.packages("dccvalidator")
install.packages"devtools"
install.packages("devtools")
devtools::install_github("Sage-Bionetworks/dccvalidator")
set.seed(123)
original_model <- forester(data   = train_data,
target = "status",
type   = "classification")
install.packages("C:/Users/kiran/Downloads/dccvalidator_0.3.0.tar.gz", repos = NULL, type = "source")
remove.packages("devtools")
